{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144384da-2472-41c1-b24d-9a7665816ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import psutil\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import einops\n",
    "import mne\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch.utils.data import Dataset as TDataset, DataLoader\n",
    "\n",
    "from datasets import DatasetDict, Dataset\n",
    "from datasets import load_dataset\n",
    "from evaluate import load as load_metric\n",
    "import scipy.special\n",
    "from comet_ml import Experiment, ExistingExperiment\n",
    "\n",
    "import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e51955-41a6-49ed-8dca-55d5d8f6773d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "else:\n",
    "    print(\"CUDA is not available, no GPUs found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce1800a-b2c0-4d49-bc1f-c901d746da29",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_memory = psutil.virtual_memory().total\n",
    "available_memory = psutil.virtual_memory().available\n",
    "print(f\"The available RAM memory is {available_memory / (1024**3):.2f} GB out of {total_memory / (1024**3):.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccd0720-a7a7-4a89-96a6-5d371a459362",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPloss(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        clip_temperature,\n",
    "        clip_temperature_type,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        if clip_temperature_type == 'param':\n",
    "            self.temperature = nn.Parameter(torch.tensor(math.log(clip_temperature), dtype=torch.float32))\n",
    "        elif clip_temperature_type == 'hparam':\n",
    "            self.temperature = torch.tensor(math.log(clip_temperature), dtype=torch.float32, requires_grad=False)\n",
    "\n",
    "    def forward(self, brainwave_embeddings, audio_embeddings):\n",
    "        batch_size = brainwave_embeddings.size(0)\n",
    "        if len(audio_embeddings.shape) == 3:\n",
    "            brainwave_embeddings = F.normalize(brainwave_embeddings, dim=(-2, -1))\n",
    "            audio_embeddings = F.normalize(audio_embeddings, dim=(-2, -1))\n",
    "            similarity = torch.einsum('Bef, bef -> Bb', brainwave_embeddings, audio_embeddings)\n",
    "        elif len(audio_embeddings.shape) == 2:\n",
    "            brainwave_embeddings = F.normalize(brainwave_embeddings, dim=(-1))\n",
    "            audio_embeddings = F.normalize(audio_embeddings, dim=(-1))\n",
    "            similarity = torch.einsum('Bf, bf -> Bb', brainwave_embeddings, audio_embeddings)\n",
    "        similarity_temperature = similarity / torch.exp(self.temperature)\n",
    "        labels = torch.arange(batch_size).to(similarity.device)\n",
    "        loss = F.cross_entropy(similarity, labels)\n",
    "        loss_temperature = F.cross_entropy(similarity_temperature, labels)\n",
    "        return loss, loss_temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f54383-3fa3-415b-8a2c-d4656f609f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "\n",
    "    def forward(self, brainwave_embeddings, audio_embeddings, temperature):\n",
    "        brainwave_embeddings = einops.rearrange(brainwave_embeddings, 'b f t -> b (f t)')\n",
    "        audio_embeddings = einops.rearrange(audio_embeddings, 'b f t -> b (f t)')\n",
    "        loss = self.mse(brainwave_embeddings, audio_embeddings)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f1e0fb-9fc4-4f47-9326-1ff91a1cf6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(brainwave_embeddings, audio_embeddings, labels):\n",
    "    if len(audio_embeddings.shape) == 3:\n",
    "        brainwave_embeddings = F.normalize(brainwave_embeddings, dim=(-2, -1))\n",
    "        audio_embeddings = F.normalize(audio_embeddings, dim=(-2, -1))\n",
    "        similarity = torch.einsum('Bef, bef -> Bb', brainwave_embeddings, audio_embeddings)\n",
    "    elif len(audio_embeddings.shape) == 2:\n",
    "        brainwave_embeddings = F.normalize(brainwave_embeddings, dim=(--1))\n",
    "        audio_embeddings = F.normalize(audio_embeddings, dim=(-1))\n",
    "        similarity = torch.einsum('Bf, bf -> Bb', brainwave_embeddings, audio_embeddings)\n",
    "    labels = labels.view(-1,1)\n",
    "    \n",
    "    index_top10 = torch.topk(similarity, 10, dim=-1).indices\n",
    "    index_top1 = torch.topk(similarity, 1, dim=-1).indices\n",
    "    is_in_top10 = torch.eq(index_top10, labels).any(dim=1)\n",
    "    is_in_top1 = torch.eq(index_top1, labels).any(dim=1)\n",
    "    return is_in_top10, is_in_top1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f47cb1-114f-4296-9425-85f06ef2b5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cart2sph(sensor_xyz):\n",
    "    x, y, z = sensor_xyz[:,0], sensor_xyz[:,1], sensor_xyz[:,2]\n",
    "    xy = np.linalg.norm(sensor_xyz[:,:2], axis=-1)\n",
    "    r = np.linalg.norm(sensor_xyz, axis=-1)\n",
    "    theta = np.arctan2(xy, z)\n",
    "    phi = np.arctan2(y, x)\n",
    "    return np.stack((r, theta, phi), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c455610-7c18-4112-8b16-0e48142a96dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatialAttentionLayer(nn.Module):\n",
    "    def __init__(self, n_input, n_output, K, coords_xy, n_dropout, dropout_radius, seed=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_input = n_input\n",
    "        self.n_dropout = n_dropout\n",
    "        self.dropout_radius = dropout_radius\n",
    "        \n",
    "        coords_xy = torch.tensor(coords_xy, dtype=torch.float32, requires_grad=False)\n",
    "        self.register_buffer('_coords_xy', coords_xy)\n",
    "\n",
    "        coords_xyz = np.load('../datasets/MASC-MEG/process_v2/coords/sensor_xyz.npy')\n",
    "        coords_sph = cart2sph(coords_xyz)\n",
    "        coords_sph = torch.tensor(coords_sph, dtype=torch.float32, requires_grad=False)\n",
    "        \n",
    "        layout = self._create_layout(coords_sph, K-1)\n",
    "        self.register_buffer('_layout', layout)\n",
    "        \n",
    "        Z = self._create_parameters(n_input, n_output, K, seed)\n",
    "        self.Z = nn.Parameter(Z)\n",
    "\n",
    "        \n",
    "    def _create_layout(self, coords_sph, L=8):\n",
    "            n_input = coords_sph.shape[0]\n",
    "\n",
    "            coords_theta = coords_sph[:,1]\n",
    "            coords_phi = coords_sph[:,2]\n",
    "            layout = torch.zeros((1, L+1, L+1, n_input), dtype=torch.float32, requires_grad=False)\n",
    "        \n",
    "            Plak_values = np.zeros((L+1, L+1, n_input))\n",
    "            for i in range (0, n_input):\n",
    "                Plak_values[:,:, i] = scipy.special.lpmn(L, L, math.cos(coords_theta[i]))[0]\n",
    "            Plak_values = torch.tensor(Plak_values, dtype=torch.float32)\n",
    "            \n",
    "            def get_factor(l, m):\n",
    "                result = math.log(2 * l + 1)\n",
    "                result -= math.log((2 if m==0 else 1) * 2 * math.pi)\n",
    "                result += scipy.special.gammaln(l - abs(m) + 1)\n",
    "                result -= scipy.special.gammaln(l + abs(m) + 1)\n",
    "                result /= 2\n",
    "                result = math.exp(result)\n",
    "                return result\n",
    "\n",
    "            counter = -1\n",
    "            for l in range (0, L+1):\n",
    "                for m in range(-l,l+1):\n",
    "                    counter += 1\n",
    "                    i, j = counter % (L+1), counter // (L+1)\n",
    "                    mult_left = get_factor(l, m)\n",
    "                    # mult_left = 1\n",
    "                    \n",
    "                    if m >= 0:\n",
    "                        mult = torch.cos(m * coords_phi)\n",
    "                    elif m < 0:\n",
    "                        mult = torch.sin(- m * coords_phi)\n",
    "                    sela = mult_left * Plak_values[abs(m), l, :] * mult\n",
    "                    \n",
    "                    layout[:,i,j,:] = sela\n",
    "            return layout\n",
    "\n",
    "    def _create_parameters(self, n_input, n_output, K, seed=None):\n",
    "        if seed is None:\n",
    "            seed = int(torch.empty((), dtype=torch.int64).random_().item())\n",
    "        generator = torch.Generator()\n",
    "        generator.manual_seed(seed)\n",
    "        \n",
    "        Z = torch.randn(size=((n_output, K, K)), generator=generator) * 2 / (n_input + n_output)\n",
    "        Z = einops.rearrange(Z, 'j k l -> j k l 1')\n",
    "        return Z\n",
    "    \n",
    "    def to(self, device):\n",
    "        self._coords_xy = self._coords_xy.to(device)\n",
    "        self._layout = self._layout.to(device)\n",
    "        return super().to(device)\n",
    "\n",
    "    def get_spatial_filter(self):\n",
    "        A = einops.reduce(self.Z * self._layout, 'j k l i -> j i', 'sum')\n",
    "        ASoftmax = F.softmax(A, dim=1)\n",
    "        return ASoftmax.clone().detach()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        A = einops.reduce(self.Z * self._layout, 'j k l i -> j i', 'sum')\n",
    "        if self.training and self.n_dropout > 0:\n",
    "            mask = torch.zeros((1, self.n_input), dtype=A.dtype, device=A.device)\n",
    "            dropout_location = torch.rand(size=(self.n_dropout, 2), device=A.device) * 0.8 + 0.1\n",
    "            for k in range(self.n_dropout):\n",
    "                for i in range(self.n_input):\n",
    "                    if torch.linalg.norm(self._coords_xy[i] - dropout_location[k]) <= self.dropout_radius:\n",
    "                        mask[:,i] = - float('inf')\n",
    "            A = A + mask\n",
    "        ASoftmax = F.softmax(A, dim=1)\n",
    "        SAx = torch.einsum('oi, bit -> bot', ASoftmax, x)\n",
    "        return SAx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a366f2-5d57-4dd5-b4c7-65dd28090d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubjectPlusLayer(nn.Module):\n",
    "    def __init__(self, n_input, n_output, n_subjects, regularize=True, bias=False, seed=None):\n",
    "        super().__init__()\n",
    "        self.bias = bias\n",
    "        self.regularize = regularize\n",
    "        if self.regularize:\n",
    "            self.regularizer = None\n",
    "        \n",
    "        A, b = self._create_parameters(n_input, n_output, n_subjects)\n",
    "        self.A = nn.Parameter(A)\n",
    "\n",
    "        I = torch.zeros((1, n_output, n_input), requires_grad=False)\n",
    "        self.register_buffer('I', I)\n",
    "        \n",
    "        if self.bias:\n",
    "            self.b = nn.Parameter(b)\n",
    "            zero = torch.zeros(size=(1, n_output, 1))\n",
    "            self.register_buffer('zero', zero)\n",
    "        \n",
    "    def _create_parameters(self, n_input, n_output, n_subjects, seed=None):\n",
    "        A = torch.zeros(size=(n_subjects, n_output, n_input))\n",
    "        b = torch.zeros(size=(n_subjects, n_output, 1)) if self.bias else None\n",
    "        with torch.no_grad():\n",
    "            for subjects in range(n_subjects):\n",
    "                layer = nn.Conv1d(in_channels=n_input, out_channels=n_output, kernel_size=1)\n",
    "                A[subjects] = einops.rearrange(layer.weight.data, 'o i 1 -> o i')\n",
    "                if self.bias:\n",
    "                    b[subjects] = einops.rearrange(layer.bias.data, 'o -> o 1')\n",
    "        return A, b\n",
    "    \n",
    "    def _create_regularizer(self, A, b):\n",
    "        batch_size = A.shape[0]\n",
    "        reg = torch.norm(A - self.I, p='fro')\n",
    "        if self.bias:\n",
    "            reg += torch.norm(b, p='fro')\n",
    "        reg = reg / batch_size\n",
    "        return reg\n",
    "    \n",
    "    def get_regularizer(self):\n",
    "        regularizer = self.regularizer\n",
    "        self.regularizer = None\n",
    "        return regularizer\n",
    "    \n",
    "    def forward(self, x, s):\n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        A = torch.cat([self.I, self.A], dim=0)\n",
    "        s[s >= A.size(0)] = 0\n",
    "        A_ = A[s,:,:]\n",
    "        out = torch.einsum('bji, bit -> bjt', A_, x)\n",
    "        \n",
    "        if self.bias:\n",
    "            b = torch.cat([self.zero, self.b], dim=0)\n",
    "            b_ = b[s,:,:]\n",
    "            out = out + b_\n",
    "        \n",
    "        if self.regularize and self.training:\n",
    "            self.regularizer = self._create_regularizer(A_, b_)\n",
    "            \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7e1e32-4fae-4e04-8829-54857a9247c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, n_input, n_output, block_index):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.kernel_size = 3\n",
    "        self.block_index = block_index\n",
    "        dilation1 = 2**(2*block_index % 5)\n",
    "        dilation2 = 2**((2*block_index + 1) % 5)\n",
    "        dilation3 = 2\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(in_channels=n_input, out_channels=n_output, kernel_size=self.kernel_size, dilation=dilation1, padding='same')\n",
    "        self.conv2 = nn.Conv1d(in_channels=n_output, out_channels=n_output, kernel_size=self.kernel_size, dilation=dilation2, padding='same')\n",
    "        self.conv3 = nn.Conv1d(in_channels=n_output, out_channels=2*n_output, kernel_size=self.kernel_size, dilation=dilation3, padding='same')\n",
    "        \n",
    "        self.batchnorm1 = nn.BatchNorm1d(n_output)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(n_output)\n",
    "        \n",
    "        self.activation1 = nn.GELU()\n",
    "        self.activation2 = nn.GELU()\n",
    "        self.activation3 = nn.GLU(dim=-2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        c1x = self.conv1(x)\n",
    "        res1 = c1x if self.block_index == 0 else x + c1x\n",
    "        res1 = self.batchnorm1(res1)\n",
    "        res1 = self.activation1(res1)\n",
    "        \n",
    "        c2x = self.conv2(res1)\n",
    "        res2 = res1 + c2x\n",
    "        res2 = self.batchnorm2(res2)\n",
    "        res2 = self.activation2(res2)\n",
    "        \n",
    "        c3x = self.conv3(res2)\n",
    "        out = self.activation3(c3x)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c18403-4030-4a58-be01-9eb48f97c27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvHead(nn.Module):\n",
    "    def __init__(self, n_channels, n_features, pool, head_stride):\n",
    "        super().__init__()\n",
    "        \n",
    "        \n",
    "        if pool == 'max':\n",
    "            self.pool = nn.Sequential(\n",
    "                nn.MaxPool1d(kernel_size=3, stride=stride, padding=0 if head_stride==2 else 1),\n",
    "                nn.Conv1d(in_channels=n_channels, out_channels=2*n_channels, kernel_size=1)\n",
    "            )\n",
    "        elif pool == 'conv':\n",
    "            self.pool = nn.Conv1d(in_channels=n_channels, out_channels=2*n_channels, kernel_size=3, stride=head_stride, padding=0 if head_stride==2 else 1)\n",
    "            \n",
    "        self.conv = nn.Conv1d(in_channels=2*n_channels, out_channels=n_features, kernel_size=1)\n",
    "        self.activation = nn.GELU()\n",
    "        self.batch_norm = nn.BatchNorm1d(n_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.conv(x)\n",
    "        x = self.batch_norm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef422ec-8e5d-4f4d-8d82-4480bb669485",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatialModule(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_input,\n",
    "        n_attantion,\n",
    "        n_unmix,\n",
    "        use_spatial_attention,\n",
    "        n_spatial_harmonics,\n",
    "        coords_xy_scaled,\n",
    "        spatial_dropout_number,\n",
    "        spatial_dropout_radius,\n",
    "        use_unmixing_layer,\n",
    "        use_subject_layer,\n",
    "        n_subjects,\n",
    "        regularize_subject_layer,\n",
    "        bias_subject_layer,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        if use_spatial_attention:\n",
    "            self.self_attention = SpatialAttentionLayer(\n",
    "                n_input, n_attantion, n_spatial_harmonics, coords_xy_scaled, spatial_dropout_number, spatial_dropout_radius\n",
    "            )\n",
    "        else: self.self_attention = None\n",
    "\n",
    "        if use_unmixing_layer:\n",
    "            n_attantion = n_attantion if self.self_attention else n_input\n",
    "            self.unmixing_layer = nn.Conv1d(in_channels=n_attantion, out_channels=n_attantion, kernel_size=1)\n",
    "        else: self.unmixing_layer = None\n",
    "            \n",
    "        if use_subject_layer:\n",
    "            n_attantion = n_attantion if (self.self_attention or self.unmixing_layer) else n_input\n",
    "            self.subject_layer = SubjectPlusLayer(\n",
    "                n_attantion, n_unmix, n_subjects, regularize=regularize_subject_layer, bias=bias_subject_layer\n",
    "            )\n",
    "        else: self.subject_layer = None\n",
    "\n",
    "    def forward(self, xs):\n",
    "        x, s = xs\n",
    "        x = self.self_attention(x) if self.self_attention else x\n",
    "        x = self.unmixing_layer(x) if self.unmixing_layer else x\n",
    "        x = self.subject_layer(x, s) if self.subject_layer else x\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db28e1d-bf9e-4c28-a708-545723e6473b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalModule(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_unmix,\n",
    "        n_block,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv_blocks = nn.ModuleDict()\n",
    "        for block_index in range(0, 5):\n",
    "            n_in = n_unmix if block_index == 0 else n_block\n",
    "            self.conv_blocks[f'conv_block_{block_index}'] = ConvBlock(n_in, n_block, block_index)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for _, module in self.conv_blocks.items():\n",
    "            x = module(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb268083-333f-4eaf-bf2f-00571b308af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BrainModule(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.spatial_module = SpatialModule(\n",
    "            n_input=kwargs[\"n_channels_input\"],\n",
    "            n_attantion=kwargs[\"n_channels_attantion\"],\n",
    "            n_unmix=kwargs[\"n_channels_unmix\"],\n",
    "            use_spatial_attention=kwargs[\"use_spatial_attention\"],\n",
    "            n_spatial_harmonics=kwargs[\"n_spatial_harmonics\"],\n",
    "            coords_xy_scaled=np.load(kwargs[\"dirprocess\"] + 'coords/coords208_xy_scaled.npy'),\n",
    "            spatial_dropout_number=kwargs[\"spatial_dropout_number\"],\n",
    "            spatial_dropout_radius=kwargs[\"spatial_dropout_radius\"],\n",
    "            use_unmixing_layer=kwargs[\"use_unmixing_layer\"],\n",
    "            use_subject_layer=kwargs[\"use_subject_layer\"],\n",
    "            n_subjects=kwargs[\"n_subjects\"],\n",
    "            regularize_subject_layer=kwargs[\"regularize_subject_layer\"],\n",
    "            bias_subject_layer=kwargs[\"bias_subject_layer\"],\n",
    "        )\n",
    "        \n",
    "        self.temporal_module = TemporalModule(\n",
    "            n_unmix=kwargs[\"n_channels_unmix\"],\n",
    "            n_block=kwargs[\"n_channels_block\"],\n",
    "        )\n",
    "\n",
    "        self.feature_projection = ConvHead(\n",
    "            kwargs[\"n_channels_block\"], \n",
    "            kwargs[\"n_features\"], \n",
    "            kwargs[\"head_pool\"],\n",
    "            kwargs[\"head_stride\"],\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, xs):\n",
    "        z = self.spatial_module(xs)\n",
    "        y = self.temporal_module(z)\n",
    "        y = self.feature_projection(y)\n",
    "        return z, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6270439-b860-4ee2-b7be-5943ee5676e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleDataset(TDataset):\n",
    "    def __init__(self, meg, hidden, df, meg_sr, meg_offset=0):\n",
    "\n",
    "        self.meg = meg\n",
    "        self.hidden = hidden\n",
    "        self.df = df\n",
    "        self.meg_sr = meg_sr\n",
    "        self.meg_offset = int(meg_offset * self.meg_sr)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        row_df = self.df.loc[index]\n",
    "        \n",
    "        subject_id, session_id, story_id = row_df['subject_id'], row_df['session_id'], row_df['story_id']\n",
    "        sbj = torch.tensor(subject_id, dtype=torch.long)\n",
    "        subject_id = str(subject_id)\n",
    "        subject_id = '0' + subject_id if len(subject_id) == 1 else subject_id\n",
    "        subset = f'subject{subject_id}_session{session_id}_story{story_id}'\n",
    "        \n",
    "        meg_start, meg_stop = row_df[f'meg{self.meg_sr}_start'], row_df[f'meg{self.meg_sr}_stop']\n",
    "        meg_start, meg_stop = meg_start + self.meg_offset, meg_stop + self.meg_offset\n",
    "        wav_index = row_df['wav_index']\n",
    "        \n",
    "        meg = torch.tensor(self.meg[subset][:,meg_start:meg_stop], dtype=torch.float32)\n",
    "        hid = torch.tensor(self.hidden[wav_index], dtype=torch.float32)\n",
    "        widx = torch.tensor(wav_index, dtype=torch.long)\n",
    "\n",
    "        return meg, sbj, hid, widx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1fea22-6f54-4bf5-b564-550eeefebbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model, hparam, experiment=None):\n",
    "        self.model = model\n",
    "        self.experiment = experiment\n",
    "        if hparam[\"loss\"] == \"clip\": self.criterion = CLIPloss(\n",
    "            clip_temperature=hparam[\"clip_temperature\"], clip_temperature_type=hparam[\"clip_temperature_type\"]\n",
    "        )\n",
    "        elif hparam[\"loss\"] == \"mse\": self.criterion = MSE()\n",
    "        \n",
    "        parameters = [\n",
    "            {'params': self.model.spatial_module.parameters(), 'lr':hparam[\"lr_fe\"], 'weight_decay':hparam[\"weight_decay\"]},\n",
    "            {'params': self.model.temporal_module.parameters(), 'lr':hparam[\"lr_fe\"], 'weight_decay':hparam[\"weight_decay\"]},\n",
    "            {'params': self.model.feature_projection.parameters(), 'lr':hparam[\"lr_fe\"], 'weight_decay':hparam[\"weight_decay\"]},\n",
    "            {'params': self.criterion.parameters(), 'lr':hparam[\"clip_temperature_lr\"], 'weight_decay':0}\n",
    "        ]\n",
    "            \n",
    "        if hparam[\"optim\"] == \"Adam\": self.optimizer = torch.optim.Adam(parameters)\n",
    "        elif hparam[\"optim\"] == \"AdamW\": self.optimizer = torch.optim.AdamW(parameters)\n",
    "        \n",
    "        self.scheduler = torch.optim.lr_scheduler.ExponentialLR(self.optimizer, gamma=hparam[\"scheduler_rate\"]) if hparam[\"use_scheduler\"] else None\n",
    "        self.save_file = hparam[\"checkpoint\"]\n",
    "\n",
    "    \n",
    "    def fit(self, dataloader_train, dataloader_val, hidden_test, nepoch=1, device_index=0):\n",
    "        hidden_test = torch.tensor(hidden_test, dtype=torch.float32).to(f'cuda:{device_index}')\n",
    "        hidden_test = einops.rearrange(hidden_test, 'b t f -> b f t')\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        self.model = self.model.to(f'cuda:{device_index}')\n",
    "        self.criterion = self.criterion.to(f'cuda:{device_index}')\n",
    "        val_loss_min = float('inf')\n",
    "        \n",
    "        for epoch in range(nepoch):\n",
    "            \n",
    "            self.model.train()\n",
    "            self.criterion.train()\n",
    "            train_loss = 0\n",
    "            pbar = tqdm(dataloader_train, leave=False)\n",
    "            for meg, sbj, hidden, _ in pbar:\n",
    "                pbar.set_description(desc=f\"train epoch {epoch}\")\n",
    "  \n",
    "                hidden = einops.rearrange(hidden, 'b t f -> b f t')\n",
    "                meg, sbj, hidden = meg.to(f'cuda:{device_index}'), sbj.to(f'cuda:{device_index}'), hidden.to(f'cuda:{device_index}')\n",
    "                _, result = self.model((meg, sbj))\n",
    "\n",
    "                _, loss_temperature = self.criterion(result, hidden)\n",
    "                \n",
    "                self.optimizer.zero_grad()\n",
    "                loss_temperature.backward()\n",
    "                self.optimizer.step()\n",
    "                train_loss += loss_temperature.detach().cpu().numpy().item() * meg.shape[0]\n",
    "                \n",
    "            train_loss /= len(dataset_train)\n",
    "            if self.experiment is not None:\n",
    "                self.experiment.log_metric(\"loss_train\", train_loss, step=epoch)\n",
    "                \n",
    "            self.optimizer.zero_grad()\n",
    "            if self.scheduler:\n",
    "                self.scheduler.step()\n",
    "            \n",
    "            \n",
    "            del meg, sbj, hidden, loss_temperature, result\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            time.sleep(1)\n",
    "            model.eval()\n",
    "            self.criterion.eval()\n",
    "            val_loss = 0\n",
    "            pbar = tqdm(dataloader_val, leave=False)\n",
    "            is_in_top10s, is_in_top1s = [], []\n",
    "            for meg, sbj, hidden, widx in pbar:\n",
    "                with torch.no_grad():\n",
    "                    pbar.set_description(desc=f\"val epoch {epoch}\")\n",
    "                    hidden = einops.rearrange(hidden, 'b t f -> b f t')\n",
    "                        \n",
    "                    meg, sbj, hidden, widx = meg.to(f'cuda:{device_index}'), sbj.to(f'cuda:{device_index}'), hidden.to(f'cuda:{device_index}'), widx.to(f'cuda:{device_index}')\n",
    "                    _, result = model((meg, sbj))\n",
    "    \n",
    "                    _, loss_temperature = self.criterion(result, hidden)\n",
    "                    val_loss += loss_temperature.detach().cpu().numpy().item() * meg.shape[0]\n",
    "    \n",
    "                    is_in_top10, is_in_top1 = metrics(result, hidden_test, widx)\n",
    "                    is_in_top10s.append(is_in_top10.detach().cpu().numpy())\n",
    "                    is_in_top1s.append(is_in_top1.detach().cpu().numpy())\n",
    "                \n",
    "            top10s = np.mean(np.concatenate(is_in_top10s)).item()\n",
    "            top1s = np.mean(np.concatenate(is_in_top1s)).item()\n",
    "            \n",
    "            val_loss /= len(dataset_test)\n",
    "            if self.experiment is not None:\n",
    "                self.experiment.log_metric(\"loss_test\", val_loss, step=epoch)\n",
    "                self.experiment.log_metric(\"top10s_test\", top10s, step=epoch)\n",
    "                self.experiment.log_metric(\"top1s_test\", top1s, step=epoch)\n",
    "\n",
    "            if val_loss < val_loss_min:\n",
    "                checkpoint_dir = \"checkpoint\"\n",
    "                if not os.path.exists(checkpoint_dir):\n",
    "                    os.makedirs(checkpoint_dir)\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                    'loss': val_loss,\n",
    "                }, f\"checkpoint/{self.save_file}.pt\")\n",
    "                val_loss = val_loss_min\n",
    "            \n",
    "            del meg, sbj, hidden, loss_temperature, result\n",
    "            torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d01e9bb-36df-4f90-9cbd-937b7e379442",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_params = {\n",
    "    \"name\": \"BrainModule\",\n",
    "    \"batch_size\": 100,\n",
    "    \"lr_fe\":3e-4,\n",
    "    \"use_scheduler\":False,\n",
    "    \"scheduler_rate\":0.95,\n",
    "    \"optim\":\"AdamW\",\n",
    "    \"weight_decay\":1e-1,\n",
    "    \"clip_temperature\":1,\n",
    "    \"clip_temperature_type\":\"param\",\n",
    "    \"clip_temperature_lr\":1e-3,\n",
    "    \"meg_sr\":100,\n",
    "    \"meg_offset\":0.15,\n",
    "    \"hidden\":\"extract_features\",\n",
    "    \"n_subjects\":27,\n",
    "    \"preprocess\":\"default\",\n",
    "\n",
    "    \"n_channels_input\":208,\n",
    "    \"n_channels_attantion\":270,\n",
    "    \"n_channels_unmix\":5,\n",
    "    \"use_spatial_attention\":True,\n",
    "    \"n_spatial_harmonics\":24,\n",
    "    \"spatial_dropout_number\":0, \n",
    "    \"spatial_dropout_radius\":0.1,\n",
    "    \"use_unmixing_layer\":True,\n",
    "    \"use_subject_layer\":True,\n",
    "    \"regularize_subject_layer\":False,\n",
    "    \"bias_subject_layer\":False,\n",
    "    \"n_channels_block\":320,\n",
    "    \"n_blocks\":5,\n",
    "    \"head_pool\":\"conv\",\n",
    "    \"n_features\":1024,\n",
    "    \"loss\":\"clip\",\n",
    "    \"head_stride\":2,\n",
    "    \n",
    "}\n",
    "\n",
    "if hyper_params[\"hidden\"] == \"extract_features\":\n",
    "    hyper_params[\"n_features\"] = 512\n",
    "if hyper_params[\"hidden\"] == 'lms':\n",
    "    hyper_params[\"n_features\"] = 120\n",
    "    hyper_params[\"head_stride\"] = 1\n",
    "\n",
    "\n",
    "dirprocess =  '../datasets/MASC-MEG/process_v2/'\n",
    "hyper_params[\"dirprocess\"] = dirprocess\n",
    "coords_xy_scaled = np.load(dirprocess + 'coords/coords208_xy_scaled.npy')\n",
    "coords_xyz = np.load(dirprocess + 'coords/sensor_xyz.npy')\n",
    "hyper_params[\"robust_scale\"] = 1\n",
    "hyper_params[\"checkpoint\"] = 'br6_sp3d_base_temperature'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8f0d12-e3d2-4665-ad77-34f25b947b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "model = BrainModule(\n",
    "    n_channels_input=hyper_params[\"n_channels_input\"],\n",
    "    n_channels_attantion=hyper_params[\"n_channels_attantion\"],\n",
    "    n_channels_unmix=hyper_params[\"n_channels_unmix\"],\n",
    "    use_spatial_attention=hyper_params[\"use_spatial_attention\"],\n",
    "    n_spatial_harmonics=hyper_params[\"n_spatial_harmonics\"],\n",
    "    dirprocess=hyper_params[\"dirprocess\"],\n",
    "    coords_xy_scaled=coords_xy_scaled,  \n",
    "    spatial_dropout_number=hyper_params[\"spatial_dropout_number\"],\n",
    "    spatial_dropout_radius=hyper_params[\"spatial_dropout_radius\"],\n",
    "    use_unmixing_layer=hyper_params[\"use_unmixing_layer\"],\n",
    "    use_subject_layer=hyper_params[\"use_subject_layer\"],\n",
    "    n_subjects=hyper_params[\"n_subjects\"],\n",
    "    regularize_subject_layer=hyper_params[\"regularize_subject_layer\"],\n",
    "    bias_subject_layer=hyper_params[\"bias_subject_layer\"],\n",
    "    n_channels_block=hyper_params[\"n_channels_block\"],\n",
    "    n_features=hyper_params[\"n_features\"],\n",
    "    head_pool=hyper_params[\"head_pool\"],\n",
    "    head_stride=hyper_params[\"head_stride\"]\n",
    ")\n",
    "\n",
    "num_trainable_params = count_parameters(model)\n",
    "print(f\"Количество обучаемых параметров: {num_trainable_params}\")\n",
    "\n",
    "subject_layer = model.spatial_module\n",
    "\n",
    "def count_layer_parameters(layer):\n",
    "    return sum(p.numel() for p in layer.parameters() if p.requires_grad)\n",
    "\n",
    "num_params_subject_layer = count_layer_parameters(subject_layer)\n",
    "print(f\"Количество обучаемых параметров в Subject Layer: {num_params_subject_layer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29304460-02cb-487d-98a4-18cd2bcc37fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_train = np.load(dirprocess + f'audio/{hyper_params[\"hidden\"]}_train4.npy')\n",
    "hidden_test = np.load(dirprocess + f'audio/{hyper_params[\"hidden\"]}_test4.npy')\n",
    "\n",
    "df_train = pd.read_csv(dirprocess + f'dataframe/df_train{hyper_params[\"n_subjects\"]}.csv')\n",
    "df_test = pd.read_csv(dirprocess + f'dataframe/df_test{hyper_params[\"n_subjects\"]}.csv')\n",
    "\n",
    "meg = dict(np.load(dirprocess + f'meg/meg{hyper_params[\"n_subjects\"]}_sr{hyper_params[\"meg_sr\"]}_{hyper_params[\"preprocess\"]}_v{hyper_params[\"robust_scale\"]}.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9dcff3a-53f5-4541-9a11-a98a177f97eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = DoubleDataset(meg, hidden_train, df_train, hyper_params[\"meg_sr\"], hyper_params[\"meg_offset\"])\n",
    "dataset_test = DoubleDataset(meg, hidden_test, df_test, hyper_params[\"meg_sr\"], hyper_params[\"meg_offset\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16422ee7-df1d-47d2-8290-d28e6a269b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_train = DataLoader(dataset_train, batch_size=hyper_params['batch_size'], shuffle=True, drop_last=True)#, generator=torch.Generator(device='cuda'))\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=hyper_params['batch_size'] // 5, shuffle=False)#, generator=torch.Generator(device='cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12cdfbee-dfe1-4c8a-9954-c5d310ee27fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = Experiment(\n",
    "    api_key=\"your_api\",\n",
    "    project_name=f'metameg{hyper_params[\"n_subjects\"]}',\n",
    "    workspace=\"Your_workspace\"\n",
    "    auto_output_logging=False,\n",
    ")\n",
    "\n",
    "experiment.log_parameters(hyper_params)\n",
    "experiment.log_code()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283c211e-c7cc-40d6-b767-a04a37756120",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BrainModule(**hyper_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ec0a0e-e948-495a-98e4-45c54f64a9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model, hyper_params, experiment=experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0b89f2-ac5b-42ea-8674-d53067657cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(f'checkpoint/{hyper_params[\"checkpoint\"]}.pt')\n",
    "\n",
    "model = model.to('cpu')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "loss = checkpoint['loss']\n",
    "\n",
    "model.eval();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
