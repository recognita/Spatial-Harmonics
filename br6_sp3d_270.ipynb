{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144384da-2472-41c1-b24d-9a7665816ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import psutil\n",
    "\n",
    "import pandas as pd\n",
    "from comet_ml import Experiment, ExistingExperiment\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import einops\n",
    "import mne\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch.utils.data import Dataset as TDataset, DataLoader\n",
    "\n",
    "from datasets import DatasetDict, Dataset\n",
    "from datasets import load_dataset\n",
    "from evaluate import load\n",
    "import scipy.special\n",
    "\n",
    "import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce1800a-b2c0-4d49-bc1f-c901d746da29",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_memory = psutil.virtual_memory().total\n",
    "available_memory = psutil.virtual_memory().available\n",
    "print(f\"The available RAM memory is {available_memory / (1024**3):.2f} GB out of {total_memory / (1024**3):.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccd0720-a7a7-4a89-96a6-5d371a459362",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPloss(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        clip_temperature,\n",
    "        clip_temperature_type,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        if clip_temperature_type == 'param':\n",
    "            self.temperature = nn.Parameter(torch.tensor(math.log(clip_temperature), dtype=torch.float32))\n",
    "        elif clip_temperature_type == 'hparam':\n",
    "            self.temperature = torch.tensor(math.log(clip_temperature), dtype=torch.float32, requires_grad=False)\n",
    "\n",
    "    def forward(self, brainwave_embeddings, audio_embeddings):\n",
    "        batch_size = brainwave_embeddings.size(0)\n",
    "        if len(audio_embeddings.shape) == 3:\n",
    "            brainwave_embeddings = F.normalize(brainwave_embeddings, dim=(-2, -1))\n",
    "            audio_embeddings = F.normalize(audio_embeddings, dim=(-2, -1))\n",
    "            similarity = torch.einsum('Bef, bef -> Bb', brainwave_embeddings, audio_embeddings)\n",
    "        elif len(audio_embeddings.shape) == 2:\n",
    "            brainwave_embeddings = F.normalize(brainwave_embeddings, dim=(-1))\n",
    "            audio_embeddings = F.normalize(audio_embeddings, dim=(-1))\n",
    "            similarity = torch.einsum('Bf, bf -> Bb', brainwave_embeddings, audio_embeddings)\n",
    "        similarity_temperature = similarity / torch.exp(self.temperature)\n",
    "        labels = torch.arange(batch_size).to(similarity.device)\n",
    "        loss = F.cross_entropy(similarity, labels)\n",
    "        loss_temperature = F.cross_entropy(similarity_temperature, labels)\n",
    "        return loss, loss_temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f54383-3fa3-415b-8a2c-d4656f609f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "\n",
    "    def forward(self, brainwave_embeddings, audio_embeddings, temperature):\n",
    "        brainwave_embeddings = einops.rearrange(brainwave_embeddings, 'b f t -> b (f t)')\n",
    "        audio_embeddings = einops.rearrange(audio_embeddings, 'b f t -> b (f t)')\n",
    "        loss = self.mse(brainwave_embeddings, audio_embeddings)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f1e0fb-9fc4-4f47-9326-1ff91a1cf6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(brainwave_embeddings, audio_embeddings, labels):\n",
    "    if len(audio_embeddings.shape) == 3:\n",
    "        brainwave_embeddings = F.normalize(brainwave_embeddings, dim=(-2, -1))\n",
    "        audio_embeddings = F.normalize(audio_embeddings, dim=(-2, -1))\n",
    "        similarity = torch.einsum('Bef, bef -> Bb', brainwave_embeddings, audio_embeddings)\n",
    "    elif len(audio_embeddings.shape) == 2:\n",
    "        brainwave_embeddings = F.normalize(brainwave_embeddings, dim=(--1))\n",
    "        audio_embeddings = F.normalize(audio_embeddings, dim=(-1))\n",
    "        similarity = torch.einsum('Bf, bf -> Bb', brainwave_embeddings, audio_embeddings)\n",
    "    labels = labels.view(-1,1)\n",
    "    \n",
    "    index_top10 = torch.topk(similarity, 10, dim=-1).indices\n",
    "    index_top1 = torch.topk(similarity, 1, dim=-1).indices\n",
    "    is_in_top10 = torch.eq(index_top10, labels).any(dim=1)\n",
    "    is_in_top1 = torch.eq(index_top1, labels).any(dim=1)\n",
    "    return is_in_top10, is_in_top1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f47cb1-114f-4296-9425-85f06ef2b5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cart2sph(sensor_xyz):\n",
    "    x, y, z = sensor_xyz[:,0], sensor_xyz[:,1], sensor_xyz[:,2]\n",
    "    xy = np.linalg.norm(sensor_xyz[:,:2], axis=-1)\n",
    "    r = np.linalg.norm(sensor_xyz, axis=-1)\n",
    "    theta = np.arctan2(xy, z)\n",
    "    phi = np.arctan2(y, x)\n",
    "    return np.stack((r, theta, phi), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c455610-7c18-4112-8b16-0e48142a96dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatialAttentionLayer(nn.Module):\n",
    "    def __init__(self, n_input, n_output, K, coords_xy, n_dropout, dropout_radius, seed=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_input = n_input\n",
    "        self.n_dropout = n_dropout\n",
    "        self.dropout_radius = dropout_radius\n",
    "        \n",
    "        coords_xy = torch.tensor(coords_xy, dtype=torch.float32, requires_grad=False)\n",
    "        self.register_buffer('_coords_xy', coords_xy)\n",
    "        \n",
    "        fourier_layout = self._create_fourier_layout(n_input, K, coords_xy)\n",
    "        self.register_buffer('_fourier_layout', fourier_layout)\n",
    "        \n",
    "        Z = self._create_parameters(n_input, n_output, K, seed)\n",
    "        self.Z = nn.Parameter(Z)\n",
    "\n",
    "        \n",
    "    def _create_fourier_layout(self, n_input, K, coords_xy):\n",
    "        coords_x = coords_xy[:,0]\n",
    "        coords_y = coords_xy[:,1]\n",
    "        fourier_layout = torch.zeros((2, K, K, n_input), requires_grad=False)\n",
    "        for k in range(K):\n",
    "            for l in range(K):\n",
    "                coords = 2 * math.pi * ((k+1) * coords_x + (l+1) * coords_y)\n",
    "                fourier_layout[0,k,l,:] = torch.cos(coords)\n",
    "                fourier_layout[1,k,l,:] = torch.sin(coords)\n",
    "        fourier_layout = einops.rearrange(fourier_layout, 'a k l i -> 1 a k l i')\n",
    "        return fourier_layout\n",
    "\n",
    "    def _create_parameters(self, n_input, n_output, K, seed=None):\n",
    "        if seed is None:\n",
    "            seed = int(torch.empty((), dtype=torch.int64).random_().item())\n",
    "        generator = torch.Generator()\n",
    "        generator.manual_seed(seed)\n",
    "        \n",
    "        Z = torch.randn(size=((n_output, 2, K, K)), generator=generator) * 2 / (n_input + n_output)\n",
    "        Z = einops.rearrange(Z, 'j a k l -> j a k l 1')\n",
    "        return Z\n",
    "    \n",
    "    def to(self, device):\n",
    "        self._coords_xy = self._coords_xy.to(device)\n",
    "        self._fourier_layout = self._fourier_layout.to(device)\n",
    "        return super().to(device)\n",
    "\n",
    "    def get_spatial_filter(self):\n",
    "        A = einops.reduce(self.Z * self._fourier_layout, 'j a k l i -> j i', 'sum')\n",
    "        ASoftmax = F.softmax(A, dim=1)\n",
    "        return ASoftmax.clone().detach()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        A = einops.reduce(self.Z * self._fourier_layout, 'j a k l i -> j i', 'sum')\n",
    "        if self.training and self.n_dropout > 0:\n",
    "            mask = torch.zeros((1, self.n_input), dtype=A.dtype, device=A.device)\n",
    "            dropout_location = torch.rand(size=(self.n_dropout, 2), device=A.device) * 0.8 + 0.1\n",
    "            for k in range(self.n_dropout):\n",
    "                for i in range(self.n_input):\n",
    "                    if torch.linalg.norm(self._coords_xy[i] - dropout_location[k]) <= self.dropout_radius:\n",
    "                        mask[:,i] = - float('inf')\n",
    "            A = A + mask\n",
    "        ASoftmax = F.softmax(A, dim=1)\n",
    "        SAx = torch.einsum('oi, bit -> bot', ASoftmax, x)\n",
    "        return SAx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd26c82-7d6a-410f-8313-7200673ba02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "C=207\n",
    "T=750\n",
    "z = torch.random (C,T,1)\n",
    "x = SpatialAttentionLayer.forward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a366f2-5d57-4dd5-b4c7-65dd28090d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubjectPlusLayer(nn.Module):\n",
    "    def __init__(self, n_input, n_output, n_subjects, regularize=True, bias=False, seed=None):\n",
    "        super().__init__()\n",
    "        self.bias = bias\n",
    "        self.regularize = regularize\n",
    "        if self.regularize:\n",
    "            self.regularizer = None\n",
    "        \n",
    "        A, b = self._create_parameters(n_input, n_output, n_subjects)\n",
    "        self.A = nn.Parameter(A)\n",
    "\n",
    "        I = torch.zeros((1, n_output, n_input), requires_grad=False)\n",
    "        self.register_buffer('I', I)\n",
    "        \n",
    "        if self.bias:\n",
    "            self.b = nn.Parameter(b)\n",
    "            zero = torch.zeros(size=(1, n_output, 1))\n",
    "            self.register_buffer('zero', zero)\n",
    "        \n",
    "    def _create_parameters(self, n_input, n_output, n_subjects, seed=None):\n",
    "        A = torch.zeros(size=(n_subjects, n_output, n_input))\n",
    "        b = torch.zeros(size=(n_subjects, n_output, 1)) if self.bias else None\n",
    "        with torch.no_grad():\n",
    "            for subjects in range(n_subjects):\n",
    "                layer = nn.Conv1d(in_channels=n_input, out_channels=n_output, kernel_size=1)\n",
    "                A[subjects] = einops.rearrange(layer.weight.data, 'o i 1 -> o i')\n",
    "                if self.bias:\n",
    "                    b[subjects] = einops.rearrange(layer.bias.data, 'o -> o 1')\n",
    "        return A, b\n",
    "    \n",
    "    def _create_regularizer(self, A, b):\n",
    "        batch_size = A.shape[0]\n",
    "        reg = torch.norm(A - self.I, p='fro')\n",
    "        if self.bias:\n",
    "            reg += torch.norm(b, p='fro')\n",
    "        reg = reg / batch_size\n",
    "        return reg\n",
    "    \n",
    "    def get_regularizer(self):\n",
    "        regularizer = self.regularizer\n",
    "        self.regularizer = None\n",
    "        return regularizer\n",
    "    \n",
    "    def forward(self, x, s):\n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        A = torch.cat([self.I, self.A], dim=0)\n",
    "        s[s >= A.size(0)] = 0\n",
    "        A_ = A[s,:,:]\n",
    "        out = torch.einsum('bji, bit -> bjt', A_, x)\n",
    "        \n",
    "        if self.bias:\n",
    "            b = torch.cat([self.zero, self.b], dim=0)\n",
    "            b_ = b[s,:,:]\n",
    "            out = out + b_\n",
    "        \n",
    "        if self.regularize and self.training:\n",
    "            self.regularizer = self._create_regularizer(A_, b_)\n",
    "            \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7e1e32-4fae-4e04-8829-54857a9247c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, n_input, n_output, block_index):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.kernel_size = 3\n",
    "        self.block_index = block_index\n",
    "        dilation1 = 2**(2*block_index % 5)\n",
    "        dilation2 = 2**((2*block_index + 1) % 5)\n",
    "        dilation3 = 2\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(in_channels=n_input, out_channels=n_output, kernel_size=self.kernel_size, dilation=dilation1, padding='same')\n",
    "        self.conv2 = nn.Conv1d(in_channels=n_output, out_channels=n_output, kernel_size=self.kernel_size, dilation=dilation2, padding='same')\n",
    "        self.conv3 = nn.Conv1d(in_channels=n_output, out_channels=2*n_output, kernel_size=self.kernel_size, dilation=dilation3, padding='same')\n",
    "        \n",
    "        self.batchnorm1 = nn.BatchNorm1d(n_output)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(n_output)\n",
    "        \n",
    "        self.activation1 = nn.GELU()\n",
    "        self.activation2 = nn.GELU()\n",
    "        self.activation3 = nn.GLU(dim=-2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        c1x = self.conv1(x)\n",
    "        res1 = c1x if self.block_index == 0 else x + c1x\n",
    "        res1 = self.batchnorm1(res1)\n",
    "        res1 = self.activation1(res1)\n",
    "        \n",
    "        c2x = self.conv2(res1)\n",
    "        res2 = res1 + c2x\n",
    "        res2 = self.batchnorm2(res2)\n",
    "        res2 = self.activation2(res2)\n",
    "        \n",
    "        c3x = self.conv3(res2)\n",
    "        out = self.activation3(c3x)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c18403-4030-4a58-be01-9eb48f97c27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvHead(nn.Module):\n",
    "    def __init__(self, n_channels, n_features, pool, head_stride):\n",
    "        super().__init__()\n",
    "        \n",
    "        \n",
    "        if pool == 'max':\n",
    "            self.pool = nn.Sequential(\n",
    "                nn.MaxPool1d(kernel_size=3, stride=stride, padding=0 if head_stride==2 else 1),\n",
    "                nn.Conv1d(in_channels=n_channels, out_channels=2*n_channels, kernel_size=1)\n",
    "            )\n",
    "        elif pool == 'conv':\n",
    "            self.pool = nn.Conv1d(in_channels=n_channels, out_channels=2*n_channels, kernel_size=3, stride=head_stride, padding=0 if head_stride==2 else 1)\n",
    "            \n",
    "        self.conv = nn.Conv1d(in_channels=2*n_channels, out_channels=n_features, kernel_size=1)\n",
    "        self.activation = nn.GELU()\n",
    "        self.batch_norm = nn.BatchNorm1d(n_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.conv(x)\n",
    "        x = self.batch_norm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef422ec-8e5d-4f4d-8d82-4480bb669485",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatialModule(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_input,\n",
    "        n_attantion,\n",
    "        n_unmix,\n",
    "        use_spatial_attention,\n",
    "        n_spatial_harmonics,\n",
    "        coords_xy_scaled,\n",
    "        spatial_dropout_number,\n",
    "        spatial_dropout_radius,\n",
    "        use_unmixing_layer,\n",
    "        use_subject_layer,\n",
    "        n_subjects,\n",
    "        regularize_subject_layer,\n",
    "        bias_subject_layer,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        if use_spatial_attention:\n",
    "            self.self_attention = SpatialAttentionLayer(\n",
    "                n_input, n_attantion, n_spatial_harmonics, coords_xy_scaled, spatial_dropout_number, spatial_dropout_radius\n",
    "            )\n",
    "        else: self.self_attention = None\n",
    "\n",
    "        if use_unmixing_layer:\n",
    "            n_attantion = n_attantion if self.self_attention else n_input\n",
    "            self.unmixing_layer = nn.Conv1d(in_channels=n_attantion, out_channels=n_attantion, kernel_size=1)\n",
    "        else: self.unmixing_layer = None\n",
    "            \n",
    "        if use_subject_layer:\n",
    "            n_attantion = n_attantion if (self.self_attention or self.unmixing_layer) else n_input\n",
    "            self.subject_layer = SubjectPlusLayer(\n",
    "                n_attantion, n_unmix, n_subjects, regularize=regularize_subject_layer, bias=bias_subject_layer\n",
    "            )\n",
    "        else: self.subject_layer = None\n",
    "\n",
    "    def forward(self, xs):\n",
    "        x, s = xs\n",
    "        x = self.self_attention(x) if self.self_attention else x\n",
    "        x = self.unmixing_layer(x) if self.unmixing_layer else x\n",
    "        x = self.subject_layer(x, s) if self.subject_layer else x\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db28e1d-bf9e-4c28-a708-545723e6473b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalModule(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_unmix,\n",
    "        n_block,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv_blocks = nn.ModuleDict()\n",
    "        for block_index in range(0, 5):\n",
    "            n_in = n_unmix if block_index == 0 else n_block\n",
    "            self.conv_blocks[f'conv_block_{block_index}'] = ConvBlock(n_in, n_block, block_index)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for _, module in self.conv_blocks.items():\n",
    "            x = module(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb268083-333f-4eaf-bf2f-00571b308af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BrainModule(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.spatial_module = SpatialModule(\n",
    "            n_input=kwargs[\"n_channels_input\"],\n",
    "            n_attantion=kwargs[\"n_channels_attantion\"],\n",
    "            n_unmix=kwargs[\"n_channels_unmix\"],\n",
    "            use_spatial_attention=kwargs[\"use_spatial_attention\"],\n",
    "            n_spatial_harmonics=kwargs[\"n_spatial_harmonics\"],\n",
    "            coords_xy_scaled=np.load(kwargs[\"dirprocess\"] + 'coords/coords208_xy_scaled.npy'),\n",
    "            spatial_dropout_number=kwargs[\"spatial_dropout_number\"],\n",
    "            spatial_dropout_radius=kwargs[\"spatial_dropout_radius\"],\n",
    "            use_unmixing_layer=kwargs[\"use_unmixing_layer\"],\n",
    "            use_subject_layer=kwargs[\"use_subject_layer\"],\n",
    "            n_subjects=kwargs[\"n_subjects\"],\n",
    "            regularize_subject_layer=kwargs[\"regularize_subject_layer\"],\n",
    "            bias_subject_layer=kwargs[\"bias_subject_layer\"],\n",
    "        )\n",
    "        \n",
    "        self.temporal_module = TemporalModule(\n",
    "            n_unmix=kwargs[\"n_channels_unmix\"],\n",
    "            n_block=kwargs[\"n_channels_block\"],\n",
    "        )\n",
    "\n",
    "        self.feature_projection = ConvHead(\n",
    "            kwargs[\"n_channels_block\"], \n",
    "            kwargs[\"n_features\"], \n",
    "            kwargs[\"head_pool\"],\n",
    "            kwargs[\"head_stride\"],\n",
    "        )\n",
    "    def forward(self, xs):\n",
    "        z = self.spatial_module(xs)\n",
    "        y = self.temporal_module(z)\n",
    "        y = self.feature_projection(y)\n",
    "        return z, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6270439-b860-4ee2-b7be-5943ee5676e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleDataset(TDataset):\n",
    "    def __init__(self, meg, hidden, df, meg_sr, meg_offset=0):\n",
    "\n",
    "        self.meg = meg\n",
    "        self.hidden = hidden\n",
    "        self.df = df\n",
    "        self.meg_sr = meg_sr\n",
    "        self.meg_offset = int(meg_offset * self.meg_sr)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        row_df = self.df.loc[index]\n",
    "        \n",
    "        subject_id, session_id, story_id = row_df['subject_id'], row_df['session_id'], row_df['story_id']\n",
    "        sbj = torch.tensor(subject_id, dtype=torch.long)\n",
    "        subject_id = str(subject_id)\n",
    "        subject_id = '0' + subject_id if len(subject_id) == 1 else subject_id\n",
    "        subset = f'subject{subject_id}_session{session_id}_story{story_id}'\n",
    "        \n",
    "        meg_start, meg_stop = row_df[f'meg{self.meg_sr}_start'], row_df[f'meg{self.meg_sr}_stop']\n",
    "        meg_start, meg_stop = meg_start + self.meg_offset, meg_stop + self.meg_offset\n",
    "        wav_index = row_df['wav_index']\n",
    "        \n",
    "        meg = torch.tensor(self.meg[subset][:,meg_start:meg_stop], dtype=torch.float32)\n",
    "        hid = torch.tensor(self.hidden[wav_index], dtype=torch.float32)\n",
    "        widx = torch.tensor(wav_index, dtype=torch.long)\n",
    "\n",
    "        return meg, sbj, hid, widx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1fea22-6f54-4bf5-b564-550eeefebbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model, hparam, experiment=None):\n",
    "        self.model = model\n",
    "        self.experiment = experiment\n",
    "        if hparam[\"loss\"] == \"clip\": self.criterion = CLIPloss(\n",
    "            clip_temperature=hparam[\"clip_temperature\"], clip_temperature_type=hparam[\"clip_temperature_type\"]\n",
    "        )\n",
    "        elif hparam[\"loss\"] == \"mse\": self.criterion = MSE()\n",
    "        \n",
    "        parameters = [\n",
    "            {'params': self.model.spatial_module.parameters(), 'lr':hparam[\"lr_fe\"], 'weight_decay':hparam[\"weight_decay\"]},\n",
    "            {'params': self.model.temporal_module.parameters(), 'lr':hparam[\"lr_fe\"], 'weight_decay':hparam[\"weight_decay\"]},\n",
    "            {'params': self.model.feature_projection.parameters(), 'lr':hparam[\"lr_fe\"], 'weight_decay':hparam[\"weight_decay\"]},\n",
    "            {'params': self.criterion.parameters(), 'lr':hparam[\"clip_temperature_lr\"], 'weight_decay':0}\n",
    "        ]\n",
    "            \n",
    "        if hparam[\"optim\"] == \"Adam\": self.optimizer = torch.optim.Adam(parameters)\n",
    "        elif hparam[\"optim\"] == \"AdamW\": self.optimizer = torch.optim.AdamW(parameters)\n",
    "        \n",
    "        self.scheduler = torch.optim.lr_scheduler.ExponentialLR(self.optimizer, gamma=hparam[\"scheduler_rate\"]) if hparam[\"use_scheduler\"] else None\n",
    "        self.save_file = hparam[\"checkpoint\"]\n",
    "\n",
    "    \n",
    "    def fit(self, dataloader_train, dataloader_val, hidden_test, nepoch=1, device_index=0):\n",
    "        hidden_test = torch.tensor(hidden_test, dtype=torch.float32).to(f'cuda:{device_index}')\n",
    "        hidden_test = einops.rearrange(hidden_test, 'b t f -> b f t')\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        self.model = self.model.to(f'cuda:{device_index}')\n",
    "        self.criterion = self.criterion.to(f'cuda:{device_index}')\n",
    "        val_loss_min = float('inf')\n",
    "        \n",
    "        for epoch in range(nepoch):\n",
    "            \n",
    "            self.model.train()\n",
    "            self.criterion.train()\n",
    "            train_loss = 0\n",
    "            pbar = tqdm(dataloader_train, leave=False)\n",
    "            for meg, sbj, hidden, _ in pbar:\n",
    "                pbar.set_description(desc=f\"train epoch {epoch}\")\n",
    "  \n",
    "                hidden = einops.rearrange(hidden, 'b t f -> b f t')\n",
    "                meg, sbj, hidden = meg.to(f'cuda:{device_index}'), sbj.to(f'cuda:{device_index}'), hidden.to(f'cuda:{device_index}')\n",
    "                _, result = self.model((meg, sbj))\n",
    "\n",
    "                _, loss_temperature = self.criterion(result, hidden)\n",
    "                \n",
    "                self.optimizer.zero_grad()\n",
    "                loss_temperature.backward()\n",
    "                self.optimizer.step()\n",
    "                train_loss += loss_temperature.detach().cpu().numpy().item() * meg.shape[0]\n",
    "                \n",
    "            train_loss /= len(dataset_train)\n",
    "            if self.experiment is not None:\n",
    "                self.experiment.log_metric(\"loss_train\", train_loss, step=epoch)\n",
    "                \n",
    "            self.optimizer.zero_grad()\n",
    "            if self.scheduler:\n",
    "                self.scheduler.step()\n",
    "        \n",
    "            del meg, sbj, hidden, loss_temperature, result\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            time.sleep(1)\n",
    "            model.eval()\n",
    "            self.criterion.eval()\n",
    "            val_loss = 0\n",
    "            pbar = tqdm(dataloader_val, leave=False)\n",
    "            is_in_top10s, is_in_top1s = [], []\n",
    "            for meg, sbj, hidden, widx in pbar:\n",
    "                with torch.no_grad():\n",
    "                    pbar.set_description(desc=f\"val epoch {epoch}\")\n",
    "                    hidden = einops.rearrange(hidden, 'b t f -> b f t')\n",
    "                        \n",
    "                    meg, sbj, hidden, widx = meg.to(f'cuda:{device_index}'), sbj.to(f'cuda:{device_index}'), hidden.to(f'cuda:{device_index}'), widx.to(f'cuda:{device_index}')\n",
    "                    _, result = model((meg, sbj))\n",
    "    \n",
    "                    _, loss_temperature = self.criterion(result, hidden)\n",
    "                    val_loss += loss_temperature.detach().cpu().numpy().item() * meg.shape[0]\n",
    "    \n",
    "                    is_in_top10, is_in_top1 = metrics(result, hidden_test, widx)\n",
    "                    is_in_top10s.append(is_in_top10.detach().cpu().numpy())\n",
    "                    is_in_top1s.append(is_in_top1.detach().cpu().numpy())\n",
    "                \n",
    "            top10s = np.mean(np.concatenate(is_in_top10s)).item()\n",
    "            top1s = np.mean(np.concatenate(is_in_top1s)).item()\n",
    "            \n",
    "            val_loss /= len(dataset_test)\n",
    "            if self.experiment is not None:\n",
    "                self.experiment.log_metric(\"loss_test\", val_loss, step=epoch)\n",
    "                self.experiment.log_metric(\"top10s_test\", top10s, step=epoch)\n",
    "                self.experiment.log_metric(\"top1s_test\", top1s, step=epoch)\n",
    "\n",
    "            if val_loss < val_loss_min:\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                    'loss': val_loss,\n",
    "                }, f\"checkpoint/{self.save_file}.pt\")\n",
    "                val_loss = val_loss_min\n",
    "            \n",
    "            del meg, sbj, hidden, loss_temperature, result\n",
    "            torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d01e9bb-36df-4f90-9cbd-937b7e379442",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_params = {\n",
    "    \"name\": \"BrainModule\",\n",
    "    \"batch_size\": 100,\n",
    "    \"lr_fe\":3e-4,\n",
    "    \"use_scheduler\":False,\n",
    "    \"scheduler_rate\":0.95,\n",
    "    \"optim\":\"AdamW\",\n",
    "    \"weight_decay\":1e-1,\n",
    "    \"clip_temperature\":1,\n",
    "    \"clip_temperature_type\":\"param\",\n",
    "    \"clip_temperature_lr\":1e-3,\n",
    "    \"meg_sr\":100,\n",
    "    \"meg_offset\":0.15,\n",
    "    \"hidden\":\"extract_features\",\n",
    "    \"n_subjects\":27,\n",
    "    \"preprocess\":\"default\",\n",
    "\n",
    "    \"n_channels_input\":208,\n",
    "    \"n_channels_attantion\":270,\n",
    "    \"n_channels_unmix\":6,\n",
    "    \"use_spatial_attention\":True,\n",
    "    \"n_spatial_harmonics\":32,\n",
    "    \"spatial_dropout_number\":0, \n",
    "    \"spatial_dropout_radius\":0.1,\n",
    "    \"use_unmixing_layer\":True,\n",
    "    \"use_subject_layer\":True,\n",
    "    \"regularize_subject_layer\":False,\n",
    "    \"bias_subject_layer\":False,\n",
    "    \"n_channels_block\":320,\n",
    "    \"n_blocks\":5,\n",
    "    \"head_pool\":\"conv\",\n",
    "    \"n_features\":1024,\n",
    "    \"loss\":\"clip\",\n",
    "    \"head_stride\":2,\n",
    "    \n",
    "}\n",
    "\n",
    "if hyper_params[\"hidden\"] == \"extract_features\":\n",
    "    hyper_params[\"n_features\"] = 512\n",
    "if hyper_params[\"hidden\"] == 'lms':\n",
    "    hyper_params[\"n_features\"] = 120\n",
    "    hyper_params[\"head_stride\"] = 1\n",
    "\n",
    "\n",
    "dirprocess =  'E:/MetaMEG/datasets/MASC-MEG/process_v2/'\n",
    "hyper_params[\"dirprocess\"] = dirprocess\n",
    "coords_xy_scaled = np.load(dirprocess + 'coords/coords208_xy_scaled.npy')\n",
    "coords_xyz = np.load(dirprocess + 'coords/sensor_xyz.npy')\n",
    "hyper_params[\"robust_scale\"] = 1\n",
    "hyper_params[\"checkpoint\"] = 'br6_sp3d_base_temperature'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29304460-02cb-487d-98a4-18cd2bcc37fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_train = np.load(dirprocess + f'audio/{hyper_params[\"hidden\"]}_train4.npy')\n",
    "hidden_test = np.load(dirprocess + f'audio/{hyper_params[\"hidden\"]}_test4.npy')\n",
    "\n",
    "df_train = pd.read_csv(dirprocess + f'dataframe/df_train{hyper_params[\"n_subjects\"]}.csv')\n",
    "df_test = pd.read_csv(dirprocess + f'dataframe/df_test{hyper_params[\"n_subjects\"]}.csv')\n",
    "\n",
    "meg = dict(np.load(dirprocess + f'meg/meg{hyper_params[\"n_subjects\"]}_sr{hyper_params[\"meg_sr\"]}_{hyper_params[\"preprocess\"]}_v{hyper_params[\"robust_scale\"]}.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9dcff3a-53f5-4541-9a11-a98a177f97eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = DoubleDataset(meg, hidden_train, df_train, hyper_params[\"meg_sr\"], hyper_params[\"meg_offset\"])\n",
    "dataset_test = DoubleDataset(meg, hidden_test, df_test, hyper_params[\"meg_sr\"], hyper_params[\"meg_offset\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16422ee7-df1d-47d2-8290-d28e6a269b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_train = DataLoader(dataset_train, batch_size=hyper_params['batch_size'], shuffle=True, drop_last=True)#, generator=torch.Generator(device='cuda'))\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=hyper_params['batch_size'] // 5, shuffle=False)#, generator=torch.Generator(device='cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12cdfbee-dfe1-4c8a-9954-c5d310ee27fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = Experiment(\n",
    "    api_key=\"Your_api\",\n",
    "    project_name=f'metameg{hyper_params[\"n_subjects\"]}',\n",
    "    workspace=\"\",\n",
    "    auto_output_logging=False,\n",
    ")\n",
    "\n",
    "experiment.log_parameters(hyper_params)\n",
    "experiment.log_code()\n",
    "# experiment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283c211e-c7cc-40d6-b767-a04a37756120",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BrainModule(**hyper_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ec0a0e-e948-495a-98e4-45c54f64a9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model, hyper_params, experiment=experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31faeecf-ab51-41d5-b1c3-44a70d196fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    trainer.fit(dataloader_train, dataloader_test, hidden_test, nepoch=20, device_index=1)\n",
    "finally:\n",
    "    if trainer.experiment:\n",
    "        experiment.end()\n",
    "    trainer.experiment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15831848-25a8-499a-86fb-0331664bde16",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(f'checkpoint/{hyper_params[\"checkpoint\"]}.pt')\n",
    "\n",
    "model = model.to('cpu')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "loss = checkpoint['loss']\n",
    "\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3b9c34-1505-45db-91eb-1764e3b7d8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_matrix = model.spatial_module.self_attention.get_spatial_filter().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22692a1e-e2ed-4340-99ce-fc6fbe0cac62",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_sum = attention_matrix.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c5a938-6f24-4764-a98f-d1432c4d74d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import mne_bids\n",
    "\n",
    "dirpath = '/mnt/datasets/MASC-MEG/bids_anonym/'\n",
    "bids_root = Path(dirpath)\n",
    "\n",
    "subject, session, task = 1, 0, 0\n",
    "subject_index = \"0\" + str(subject) if len(str(subject)) == 1 else str(subject)\n",
    "bids_path = mne_bids.BIDSPath(\n",
    "    subject=subject_index,\n",
    "    session=str(session),\n",
    "    task=str(task),\n",
    "    datatype=\"meg\",\n",
    "    root=bids_root,\n",
    ")\n",
    "\n",
    "try:\n",
    "    raw = mne_bids.read_raw_bids(bids_path, verbose=False)\n",
    "except FileNotFoundError:\n",
    "    print(\"missing\", subject, session, task)\n",
    "\n",
    "raw = raw.pick_types(\n",
    "    meg=True, misc=False, eeg=False, eog=False, ecg=False\n",
    ")\n",
    "\n",
    "layout = mne.channels.find_layout(raw.info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde77068-32a9-4426-93f0-cb2f7e340cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(4, 4))\n",
    "ax = np.asarray([[ax]])\n",
    "\n",
    "mne.viz.plot_topomap(attention_sum, (layout.pos -0.5)*2, show=False, axes=ax[0,0], sphere=1)\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(f'image2/br6_sa3d_base_attention')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e85323-097f-4cda-a9e8-20c53b6a7670",
   "metadata": {},
   "outputs": [],
   "source": [
    "U, S, Vh = np.linalg.svd(attention_matrix)\n",
    "attention_svd = Vh[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27159ab2-5b78-4c34-8b17-8c4ac18855a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313b0a0b-beb6-434b-bd2c-47647c30fc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 3, figsize=(9, 6))\n",
    "\n",
    "for j in range(3):\n",
    "    for i in range(2):\n",
    "        mne.viz.plot_topomap(Vh[i*3+j], (layout.pos -0.5)*2, show=False, axes=ax[i,j], sphere=1)\n",
    "        ax[i,j].set_title(f'attention (svd), {i*3+j}')\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(f'image2/attention (svd)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0242d792-9e4a-497a-9ef1-fc19ebb402cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CWTLayer(nn.Module):\n",
    "    def __init__(self, freq_low=2, freq_high=40, n_freq=40, srate=200, freq_type='log', n_cycles=7.0, n_sigma=5, n_cycles_type='fixed', q=1, filt_norm=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.srate = srate\n",
    "        self.n_sigma = n_sigma\n",
    "        self.n_cycles_type = n_cycles_type\n",
    "        self.bias = math.log(math.exp(n_cycles) - 1)\n",
    "        self.q = q\n",
    "        self.filt_norm = filt_norm\n",
    "        \n",
    "        self.register_buffer('_freqs', torch.logspace(math.log10(freq_low), math.log10(freq_high), steps=n_freq))\n",
    "\n",
    "        n_cycles = torch.tensor(n_cycles, dtype=torch.float32)\n",
    "        if self.n_cycles_type == 'multiple':\n",
    "            n_cycles = n_cycles.repeat(n_freq)\n",
    "        ln_cycles = torch.log(torch.exp(n_cycles) - 1) - self.bias\n",
    "    \n",
    "        if self.n_cycles_type == 'fixed':\n",
    "            self.register_buffer('_ln_cycles', ln_cycles)\n",
    "        elif (self.n_cycles_type == 'single') or (self.n_cycles_type == 'multiple'):\n",
    "            self._ln_cycles = nn.Parameter(ln_cycles)\n",
    "        \n",
    "        self.softplus = nn.Softplus()\n",
    "\n",
    "        \n",
    "    def _create_filters(self):\n",
    "        n_cycles = self.softplus(self._ln_cycles + self.bias)\n",
    "        sigma = n_cycles / (2.0 * math.pi * self._freqs)\n",
    "        sigma = sigma.reshape((-1,1,1,1))\n",
    "        \n",
    "        n_timestemps = (self.n_sigma * torch.max(sigma) * self.srate).to(int).item()\n",
    "        scale = torch.arange(-n_timestemps, n_timestemps+1, device=self._ln_cycles.device) / self.srate\n",
    "        scale = scale.reshape((1,1,1,-1))\n",
    "        \n",
    "        freqs = self._freqs.reshape((-1,1,1,1))\n",
    "\n",
    "        scaling_factor = (2 * math.pi)**(-1/2) * sigma / (self.srate / 2)\n",
    "        oscillation = torch.exp(1j*2*math.pi * freqs * scale) - torch.exp(-0.5*(2*math.pi * freqs)**2)\n",
    "        gaussian_envelope = torch.exp(- scale**2 / (2.0 * sigma**2))\n",
    "\n",
    "        filt = scaling_factor * oscillation * gaussian_envelope\n",
    "        if self.filt_norm:\n",
    "            filt /= math.sqrt(0.5) * torch.linalg.norm(filt, dim=-1, keepdim=True)\n",
    "        return filt, n_timestemps   \n",
    "        \n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        filt, n_timestemps = self._create_filters()\n",
    "        x = F.pad(x, pad=(n_timestemps, n_timestemps, 0, 0), mode='constant', value=0)\n",
    "        x = x.to(torch.complex64)\n",
    "        x = F.conv2d(x, filt, padding='valid', stride=self.q)\n",
    "        x = torch.abs(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6044a06a-c270-4bec-962b-924cf355ec7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "eeg_id = 1001369401\n",
    "path_fif = f'{eeg_id}_eeg.fif'\n",
    "\n",
    "raw = mne.io.read_raw_fif(path_fif, preload=True)#, verbose=False)\n",
    "start_seconds, stop_seconds = 20, 30\n",
    "sfreq = raw.info['sfreq']\n",
    "data = raw.get_data(picks=['eeg'], start=round(start_seconds * sfreq), stop=round(stop_seconds * sfreq))\n",
    "epoch_data = np.expand_dims(data, 0) * 1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2ec589-25ab-4e80-800f-60f780d60ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fixed, single, multiple\n",
    "cwt = CWTLayer(\n",
    "    freq_low=2, \n",
    "    freq_high=40, \n",
    "    n_freq=40, \n",
    "    srate=200, \n",
    "    freq_type='log', \n",
    "    n_cycles=7.0, \n",
    "    n_sigma=5,\n",
    "    n_cycles_type='fixed',\n",
    "    q=1,\n",
    "    filt_norm=True,\n",
    ").to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30dd0a23-5128-4255-a150-4bc29fe4e288",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_data_torch = torch.tensor(epoch_data, dtype=torch.float32).to('cuda:0')\n",
    "amplitude = cwt(epoch_data_torch)\n",
    "logpower = 2 * torch.log(torch.clip(amplitude, min=torch.finfo().eps))\n",
    "print(torch.norm(torch.abs(logpower)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
